---
layout: post
current: post
cover:  assets/images/posts/2019-08-20-resnext/01.png
navigation: True
title: ResNeXt:Aggregated Residual Transformations for Deep Neural Networks
date: 2019-08-20 15:30:00
tags: [paper-review]
class: post-template
subclass: 'post'
author: soyeol
---

# ResNeXt:Aggregated Residual Transformations for Deep Neural Networks

안녕하세요! **AiRLab**(한밭대학교 인공지능 및 로보틱스 연구실) 이소열입니다!

이번에 소개할 논문은 ResNet의 변형 형태인 Aggregated Residual Transformations for Deep Neural Networks(ResNeXt)입니다!

## 1. Introduction

비전인식에 대한 연구는 "feature engineering"에서 "network engineering"으로 변화하는 추세입니다. 따라서 feature가 수작업으로 만들어지는것이 아닌, model의 architecture를 만드는것으로 옮겨지고 있습니다.

![cover](/assets/images/posts/2019-08-20-resnext/01.png)

< ResNeXt의 Basic 블록 구조 >

하지만, architecture를 디자인하는것은(특히 layer의 층이 두터워질 때) hyper-parameters의 증가로 그 난이도가 어려워지고있습니다.

같은 모양의 여러 블록을 쌓는 VGG network(2015)처럼, ResNets(2016)도 VGG와 같은 방식을 계승했고, 이 간단한 rule은 hyper parameter의 선택을 보다 간단하게 만들어주었습니다.

VGG-nets와 달리, Inception model들은 carefully하게 디자인된 방식들은 낮은 연산량으로도 높은 정확도를 이끌어 낼 수 있다고 증명했습니다. Inception model들은 계속 발전하고 있지만, 메인 아이디어는 split-transform-merge(분할, 변형, 병합) strategy입니다.

이 논문에서는, 하나의 입력을 Group convolution을 통해 여러개로 나누고, 1x1 convolution으로 입력을 transform하고, concat을 통해 merge를 진행합니다.

또한, 이 모델은 기존의 ResNet보다 연산량은 줄이면서 더 높은 성능을 보였습니다.

## 2. Method

![figure1](/assets/images/posts/2019-08-20-resnext/02.png)

< ResNet과 ResNeXt의 기본 구성 >

논문에 작성되어있는 ResNet-50과 ResNeXt-50의 구성입니다. 표에서 보시면, 각 conv layer를 지날 때 마다, output의 크기가 1/2로 줄어드는것을 볼 수 있습니다. ResNet에서는 하나의 convolution을 깊은 채널로 만든 것을 볼 수 있는 반면에, ResNeXt에서는 조금 더 깊지만 32개의 group convolution을 통해 연산량을 크게 낮춘것을 볼 수 있습니다. 

이 논문에서 나타나있는 C는 cardinarity로, Group convolution의 수로 볼 수 있습니다.

![figure2](/assets/images/posts/2019-08-20-resnext/03.png)

< Basic block 구성 >


이전의 ResNet에서는, ResNet50 이하의 깊이를 갖는 구조에서는 Basic block, 즉 블록을 하나 쌓을 때, convolution을 2개만 진행을 했었습니다. 하지만 ResNeXt에서는, 2개의 블록만 쌓게 된다면 group convolution의 의미가 없어져 성능 향상에 의미가 없게 됩니다. 따라서 ResNeXt에서는 block의 depth가 3 이상일 때 부터 성능이 향상된다고 합니다.

## 3. Implemntation details

이 논문에서는 성능 실험을 위해 ImageNet dataset를 사용했습니다. input image 를 224x224 random crop하였으며, shortcut connection을 위해서는 identity connection을 사용했습니다. (dimension이 변경될 떄는, projection shortcut 을 사용했습니다. ResNet 논문의 Option B를 참고하시면 됩니다.) 또한, Downsampling은 convolution 3,4,5layer에서 진행되었으며, 각 layer의 첫번째 블록에서 stride=2를 통해 진행하였습니다. 

SGD optimizer, mini-batch 256, 8GPU를 사용했으며, weight decay=0.0001, momentum=0.9를 넣었습니다. learning rate는 0.1에서 시작하였으며, 학습동안 3번 learning rate를 1/10로 감소시켰습니다. 또한 weight initialization을 사용했습니다.

모든 convolution 이후에는 BatchNormalization을 수행하였고, 그 이후에는 ReLU를 붙였습니다. (Shortcut 이후에는 ReLU를 적용하지 않았습니다)

## 4. Experiments

### Cardinality vs. Width

ResNeXt의 가장 큰 특징이라고 한다면 Group convoluition입니다. 이 Group의 수를 cardinality라고 하는데, group의 수를 늘릴수록 더 낮은 연산량을 가질 수 있습니다. 따라서, 같은 연산량을 갖는 네트워크라고 하면, group을 늘리면 더 깊은 채널(총합)을 가질 수 있습니다.

이 논문에서는, cardinality C와 bottleneck width의 trade-off를 비교를 해보았습니다.

![table1](/assets/images/posts/2019-08-20-resnext/04.png)

<cardinality와 width>

위의 표는, 파라미터를 일정 수준으로 유지하면서 cardinality와 block width를 변경해본 표 입니다. group의 수를 늘리면 더 많은 channel을 이용할 수 있습니다.

![table2](/assets/images/posts/2019-08-20-resnext/05.png)

< ImageNet 결과 >

그 결과를 비교한 표 입니다. 결과만 말씀드리면, 같은 연산량을 유지할 때, cardinality를 늘리는 것이 성능 향상에 큰 영향이 있었습니다. 또한, 여러가지 dataset에서의 실험을 통해, increasing cardinality의 성능 증명을 볼 수 있습니다.

## References

- He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.

- Xie, Saining, et al. "Aggregated residual transformations for deep neural networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
