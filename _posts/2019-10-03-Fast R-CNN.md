---
layout: post
current: post
cover:  assets/images/posts/2019-10-04-Fast-R-CNN/figure1.png
navigation: True
title: Fast-R-CNN
date: 2019-10-03 15:00:00
tags: [paper-review]
class: post-template
subclass: 'post'
author: daehan
---

Fast-R-CNN Review

안녕하세요. **AiRLab**(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다. 지난번 R-CNN 에 이어서 Fast-R-CNN 을 Review 해보려고 합니다.. ^_^

이번에 읽은 논문은 **Fast-R-CNN**([arXiv:1504.08083](https://https://arxiv.org/abs/1504.08083))입니다.

<hr>
#### **<u>Introduciton</u>** 

당시 deep ConvNet[14,16]이 발전하면서 image classification과 object detection의 Acc가 상당히 향상 되었다고 합니다. <br>
당연히 classification보다 object detection이 복잡합니다. 때문에, [9,11,19,25]는 느리고 비효율적인 multi-stage pipline model을 train합니다.<br>
[14,16]은 Alex_net과 Backpropagation입니다.<br>
[9,11,19,25]는 R-CNN,SPP,Overfeat,segDeepM입니다.

**<u>Multi-stage_pipline</u>**<br>
R-CNN에서 3개의 module을 따로 학습 해야하는 것을 생각하면 될 것 같습니다.<br>

**논문에서는** detection task가 object의 정밀한 localization을 필요로 하기 때문에 <u>두가지 issue</u>가 생긴다고 합니다.<br>

**[Issue1]** : 수많은 객체 위치 후보들을 처리해야한다.<br>
이는 Speed,accuracy,simplicification을 손상시킵니다.<br>
**[Issue2]** : 정확한 localization을 위해 rough_localization을 다듬어야한다.<br>

**때문에 논문에서는** [9,11]Network를 base로 하여 train과정을 단순화 합니다. (Sigle-stage traing algorthm을 제안합니다.) 결과적으로, R-CNN,SPP_net보다 VGGnet을 각각 9배,3배 빠르게 학습합니다.

**<u>R-CNN의 단점</u>**<br>
1. Multi-stage pipline(train)(Region proposal,SVM,bounding-box)
2. SVM과 bounding-box regressor의 경우(train) 각 image의 proposal의 feature들이 disk에 기록됩니다. (VGG16의 경우 2.5일이 소요됩니다._VOC07)
3. object detection이 느리다.

**논문에서는** Fast-R-CNN을 설명하기전, R-CNN과 SPPnet을 비교합니다.
1. SPPnet은 sharing compute를 이용하여 R-CNN의 속도를 높이기 위해 제안되었다.
-   여기서 말하는 sharing compute는 R-CNN과 달리 SPPnet은 한번의 convnet을 통과한 feature를 이용해 detection을 함으로써, end-to-end 학습이 된다는 이야기를 하는 것 입니다.
2. 결과적으로, test:10~100배, train:3배 빠릅니다. <br>

**그러나**, SPPnet도 주목할만한 **단점**이 있습니다. (drawback) 
1. SPPnet도 R-CNN과 마찬가지로, train은 multi-stage pipline(feature extraction/fine-tune(log_loss)/SVM train, bounding-box regressors 를 가집니다.) 또, feature가 disk에 기록됩니다.
2. SPPnet에서 제안된 algorithm은 convolutional layer를 update할수 없고 convolution layer 전에 SPP를 할 수 없다.(network perpomance를 제한한다.)<br>

**<u>Contributions</u>**<br>

<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure4.png" width='1000' hight='300'></center> 
<center>(Fast-R-CNN)</center><br>

1. image의 region_proposal을 crop&warp한 R-CNN과 달리 Fast-R-CNN은 입력이미지와 RoI_projection(object proposal)을 입력으로 받는다.
2. network를 통해 convfeature map을 생성하기위해 전체 이미지를 처리한다.
3. 각 object proposal에 대해 RoIPooling layer가 고정길이 vector를 추출한다. (각 feature vector는 fc_layer에 연속적으로 전달된다.)

**<u>Multi-task loss</u>**<br>
<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure10.png" width='1000' hight='300'></center> 
<center>(Multi-task loss)</center><br>

**<u>RoI pooling layer</u>**<br>
입력이미지에 한번만 CNN을 적용하고 RoI pooling을 이용하여 object 판별을 위한 feature를 추출하자는 것이 핵심이다.<br>
-  RoI로 추출할 feature size = HxW(eg.7x7)
-  Feature map 위에 RoI의 좌표 (r,c,h,w)
<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure7.gif" width='678' hight='300'></center> 
<center>(RoI pooling layer)</center><br>
Feature map위에서 h/H x w/W 만큼 Grid를 만들어 pooling 하면 결과적으로 원하는 HxW (첫 fc-layer와 호환이 되도록 하는 사이즈)feature size가 됩니다.<br>
(서로 다른 size의 region proposal이 입력되더라도 같은 size의 stride로 7x7을 만들어준다.)

**<u>Initializing from pre-trained networks</u>**<br>

논문에서는 pre-train된 ImageNet networks를 각각 실험합니다. pre-train network가 Fast-R-CNN을 initialize할때 3가지의 transform을 거칩니다.<br>
1. 마지막 maxpooling layer를 RoI pooling layer로 대체 한다.
2. network_last fully connected layer와 softmax layer는  two sibling layer로 대체됩니다.
3. network는 two input 을 받도록 합니다. (image list와 해당 image의 RoI 목록)

**<u>Mini-batch sampling</u>**<br>

fine-tuning에서, N=2(image),R=128 로 하여 R/N으로 image당 64RoI를 뽑습니다. CNN연산량을 줄이는 효과가 있습니다. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 다음 그림과 같은 방식을 hierarchical sampling이라고 합니다.(계층적 샘플링)
Ground-truth와 IoU > 0.5 (Positive), [0.1,0.5] 일 경우 Negative로 구분합니다. (즉, IoU가 너무 낮은것은 sample로 추가하지 않는다.) 
//train 할때, image horizontally flipped(p=0.5)의 augmentation만 사용하고 다른 augmentation 은 사용하지 않았다고 합니다. //
<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure6.png" width='678' hight='300'></center> 
<center>(Mini-batch sampling)</center><br>

#### <u>Scale invariance</u><br>
논문에서 Scal invariance 를 해결하기 위해 2가지 방법을 사용하였습니다.<br>

**[brute force learning]**<br>
train/test 에서 미리 정해진 픽셀 크기로 처리된다. 때문에, network는 train_data 로 부터 정해진 scale의 detection을 해야합니다.<br>

**[using image pyramids]**<br>
image pyramid를 통해 network는 scale invariance를 제공한다. Test 에서는 image pyramid는 각region_proposal을 nomalize하는데 사용됩니다.<br>

논문에서는 GPU memory issue로 인하여 소규모 네트워크에 대해서만 multi_scale train을 하였다고 합니다. 즉, multi_scale에 관한 실험은 별로 진행하지 않았다는 것이며 논문의 내용은 scale invariance에 더 초점을 맞추고 있습니다.
<br>

#### <u>Conclusion</u>
저자가 말한대로 Fast-R-CNN은 R-CNN과 SPPnet과 다르게 깔끔함을 강조하고 있다.
Fast-R-CNN의 특징은 다음과 같다. <br>

1. 이미지당 CNN을 한번만 수행한다.
2. RoI pooling layer의 도입.(마지막 max-pooling 대신 사용되었다.)
3. Classification 에서 SVM을 사용하던 것 과 달리 Softmax로 변경되었다. 즉, 하나의 network가 된것이다.
4. Multitask loss 에서 보면 bounding Box regressor 또한 network의 부분이 되었다. <br>

**이러한 특징을 바탕으로** , R-CNN/SPPnet과 달리 mAP가 높고, multi-task loss 를 이용하여 single-satge train이 가능하고, end-to-end학습이 가능하고 feature caching을 하지 않으므로 disk 공간을 필요로 하지 않는다는 장점이 있다. 

Fast-R-CNN의 다음 버전인, Faster-R-CNN은 region proposal 생성 방식의 개선을 주 논점으로 다룹니다.

## References
- R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.

