---
layout: post
current: post
cover:  assets/images/posts/2019-07-24-rkd/cover.png
navigation: True
title: Relational Knowledge Distillation
date: 2019-07-28 13:00:00
tags: [paper-review]
class: post-template
subclass: 'post'
author: jaemin
---

Relational Knowledge Distillation

안녕하세요. **AiRLab**(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!

오늘 소개할 논문은 Relational Knowledge Distillation(RKD) ([arXiv:1904.05068](https://arxiv.org/abs/1904.05068))이며, CVPR2019에 Accepted 된 논문입니다.


### Knowledge distillation

Knowledge distillation은 Teacher Model이 Student Model에게 Knowledge를 transferring 하는 것 인데요, 이러한 연구의 대표적인 연구는 Hinton의 `Distilling the Knowledge in a Neural Network`가 있습니다. 

이러한 논문의 시작은 computing resources의 양을 줄이이 위해 시작이 되었는데요,

만약 모델의 ACC를 높이기 위해서는 모델을 크게 설계를 하면 되지만 모델이 커질 수록 파라미터의 양은 증가하고 더 많은 computing resource를 차지할 것입니다. 이러한 문제는 computing resource를 감당할 수 있는 인프라를 가 없는 사람들에게는 큰 이슈로 남게되고, 특히 서비스로 배포가 되어야하는 모델들은 경량화 되어야 한다는 조건이 많이 붙습니다.

이러한 이슈로 큰 모델이 학습한 정보를 `증류(distillation)`하여, 작은 모델에 주입할 수 있는 Knowledge distillation가 시작되었습니다.

<img src="/assets/images/posts/2019-07-24-rkd/figure2.png" width="80%" alt="figure2" />

위와 같은 Softmax를 사용하는 네트워크가 있다고 해봅시다. 그러면 one hot으로 작성된 정답 라벨(hard label)과 predict을 비교하여 backward를 하게 됩니다.

Teacher Model은 위와 같은 프로세스로 학습을 진행하고 큰 모델이기 때문에 좋을 ACC를 만들어 낼 것입니다. 그리고 Student Model은 Teacher Model의 softmax를 거치고 나온 0과 1사이의 확률 값을 soft label로 하여 학습하는 방법이 KD(Knowledge Distillation)입니다.

### Relational Knowledge Distillation

RKD(Relational Knowledge Distillation)는 Knowledge Distillation에서 Knowledge를 어떻게 정의할 것 인가에서 시작이 되었습니다.

<img src="/assets/images/posts/2019-07-24-rkd/figure1.png" width="80%" alt="figure1" />

`t`를 각 Input에서 나오는 Teacher의 output, `s`를 Student 의 output이라고 했을 때 기존의 KD는 각각의 t가 s에 point-to-point로 연결되는 위 그림과 매칭됩니다. 때문에 이 논문에서는 기존의 KD를 IKD(Individual Knowledge Distillation)라고 표현합니다.

IKD는 Individual한 output을 Knowledge로 하여 Distillation 한다고 하면, RKD는 output들의 관계(Relation)를 Knowledge로 하여 Distillation합니다.

<img src="/assets/images/posts/2019-07-24-rkd/figure3.png" width="80%" alt="figure3" />

이 연구에서 이야기 하는 관계는 각 output이 embedding space에 표현될 때의 node간의 distance와 angle입니다.

<img src="/assets/images/posts/2019-07-24-rkd/figure4.png" width="60%" alt="figure4" />
<img src="/assets/images/posts/2019-07-24-rkd/figure5.png" width="60%" alt="figure5" />

distance는 위의 식으로 표현 될 수 있고 뮤는 node간 거리의 평균이라고 이해하면 될 것 같습니다. 따라서 아래와 같은 식을 otim하는 것으로 학습 할 수 있습니다.

<img src="/assets/images/posts/2019-07-24-rkd/figure6.png" width="60%" alt="figure6" />

angle도 아래와 같이 표현이 가능합니다.

<img src="/assets/images/posts/2019-07-24-rkd/figure7.png" width="60%" alt="figure7" />

<img src="/assets/images/posts/2019-07-24-rkd/figure8.png" width="60%" alt="figure8" />


### Experiment

이 논문에서는 여러 Task(Metric learning, Image classification, Few-shot learning) 에서 위에서 소개한 distance-wise, angle-wise 를 통해 학습한 결과를 다른 네트워크와 비교하고 있습니다.

**Metric learning**

<img src="/assets/images/posts/2019-07-24-rkd/figure9.png" width="100%" alt="figure9" />

**Image classification**

<img src="/assets/images/posts/2019-07-24-rkd/figure10.png" width="60%" alt="figure10" />

**Few-shot learning**

<img src="/assets/images/posts/2019-07-24-rkd/figure10.png" width="60%" alt="figure10" />

그 결과 모든 실험에서 가장 좋은 성능을 보였으며, 네트워크가 깊어짐에 따라서 큰 폭으로 성능 향상이 발생하는 것을 보였습니다.

## References
- Park, Wonpyo, et al. "Relational Knowledge Distillation." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.

- Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arXiv preprint arXiv:1503.02531 (2015).
